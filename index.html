<!DOCTYPE html>
<html>
<head>
    <title>ONNX Runtime JavaScript Example: Quick Start</title>
    <meta http-equiv="Cross-Origin-Opener-Policy" content="same-origin">
    <meta http-equiv="Cross-Origin-Embedder-Policy" content="require-corp">
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/npyjs"></script>
    <script src="https://unpkg.com/wasm-feature-detect/dist/umd/index.js"></script>
</head>
<body>
    <button id="load">Load Model</button>
    <script>
        let diffusionSession, decoderSession;
        const audioQueue = [];
        let isInferenceRunning = false;
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();

        document.querySelector('#load').addEventListener('click', runModel);

        async function runModel() {
            if (isInferenceRunning) {
                console.log("Inference already running, skipping...");
                return;
            }

            try {
                isInferenceRunning = true;
                const sessionOptions = {
                    executionProviders: [
                        { name: 'webnn', deviceType: 'gpu', powerPreference: "high-performance" },
                        "wasm"
                    ],
                };

                console.log("Loading diffusion model...");
                diffusionSession = await ort.InferenceSession.create(
                    'https://github.com/mdl262/RainLDM/releases/download/distilled_q/distilled_q.onnx',
                    sessionOptions
                );

                console.log("Running diffusion model...");
                const diffusionFeeds = { /* Define your input feeds here */ };
                const diffusionOutput = await diffusionSession.run(diffusionFeeds);

                console.log("Loading decoder model...");
                decoderSession = await ort.InferenceSession.create(
                    'https://github.com/mdl262/RainLDM/releases/download/decoder_q_gpu/decoder_q_gpu.onnx',
                    sessionOptions
                );

                console.log("Running decoder model...");
                const decoderFeed = { z_quantized: diffusionOutput.z };
                const decoderOutput = await decoderSession.run(decoderFeed);

                const audioData = new Float32Array(decoderOutput.s.data);
                if (audioData.length > 0) {
                    console.log("Pushing audio data to queue...");
                    await pushAudioToQueue(audioData);
                } else {
                    console.error("Error: Audio data shape is invalid.");
                }
            } catch (e) {
                console.error("Inference failed:", e);
            } finally {
                // Only release when everything is done
                if (decoderSession) await decoderSession.release();
                if (diffusionSession) await diffusionSession.release();
                isInferenceRunning = false;
            }
        }

        async function pushAudioToQueue(audioData) {
            console.log(`Audio queue length: ${audioQueue.length}`);
            if (audioQueue.length >= 5) {
                console.log("Waiting for space in audio queue...");
                await waitForSpaceInQueue();
            }
            audioQueue.push(audioData);
            console.log("Audio data pushed to queue. Queue length now:", audioQueue.length);

            if (audioQueue.length === 1) {
                playNextInQueue();
            }
        }

        async function waitForSpaceInQueue() {
            return new Promise(resolve => {
                const interval = setInterval(() => {
                    if (audioQueue.length < 5) {
                        clearInterval(interval);
                        resolve();
                    }
                }, 100); // Check every 100 ms
            });
        }

        function playNextInQueue() {
            if (audioQueue.length === 0) return;

            const audioData = audioQueue.shift();
            console.log("Playing audio from queue. Remaining items:", audioQueue.length);

            const buffer = audioContext.createBuffer(1, audioData.length, audioContext.sampleRate);
            buffer.copyToChannel(audioData, 0);

            const source = audioContext.createBufferSource();
            source.buffer = buffer;
            source.connect(audioContext.destination);
            source.onended = () => {
                console.log("Audio finished playing.");
                if (audioQueue.length > 0) {
                    playNextInQueue();
                } else {
                    console.log("Queue is empty.");
                }
            };
            source.start();
        }
    </script>
</body>
</html>
